{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa05864",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c75e250",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Imports\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Imports\n",
    "# -------------------------\n",
    "import nltk\n",
    "import tiktoken\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "CHUNK_SIZE = 100\n",
    "CHUNK_OVERLAP = 20\n",
    "ENCODING_NAME = \"cl100k_base\"\n",
    "TEXT_COLUMN = \"customer_feedback\"   # change if needed\n",
    "\n",
    "# -------------------------\n",
    "# Initialize\n",
    "# -------------------------\n",
    "nltk.download(\"punkt\")\n",
    "sent_tokenizer = nltk.sent_tokenize\n",
    "encoder = tiktoken.get_encoding(ENCODING_NAME)\n",
    "\n",
    "# -------------------------\n",
    "# Utility Functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "# -------------------------\n",
    "# Chunk Builder\n",
    "# -------------------------\n",
    "def build_chunks(text: str) -> List[str]:\n",
    "    sentences = sent_tokenizer(clean_text(text))\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # Very long sentence\n",
    "        if sentence_tokens > CHUNK_SIZE:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            chunks.append(sentence)\n",
    "            current_chunk = []\n",
    "            current_tokens = 0\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sentence_tokens <= CHUNK_SIZE:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # overlap\n",
    "            overlap_tokens = encoder.encode(\" \".join(current_chunk))[-CHUNK_OVERLAP:]\n",
    "            overlap_text = encoder.decode(overlap_tokens)\n",
    "\n",
    "            current_chunk = [overlap_text, sentence]\n",
    "            current_tokens = count_tokens(\" \".join(current_chunk))\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# -------------------------\n",
    "# Stratified Sampling\n",
    "# -------------------------\n",
    "def create_stratified_sample(csv_path, sample_size, stratify_col=\"Product\", random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    frac = sample_size / len(df)\n",
    "\n",
    "    return (\n",
    "        df.groupby(stratify_col, group_keys=False)\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=random_state))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Load & Prepare Data\n",
    "# -------------------------\n",
    "DATA_PATH = r\"C:\\Users\\hakimam\\Desktop\\pproject\\test5\\data\\processed\\filtered_feedback.csv\"\n",
    "\n",
    "df_sampled = create_stratified_sample(DATA_PATH, sample_size=200)\n",
    "\n",
    "complaints = df_sampled[TEXT_COLUMN].dropna().tolist()\n",
    "\n",
    "# -------------------------\n",
    "# Apply Chunking\n",
    "# -------------------------\n",
    "all_chunks = []\n",
    "\n",
    "for i, complaint in enumerate(complaints):\n",
    "    chunks = build_chunks(complaint)\n",
    "\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"complaint_id\": f\"COMP{i+1}\",\n",
    "            \"chunk_index\": j,\n",
    "            \"chunk_text\": chunk,\n",
    "            \"token_count\": count_tokens(chunk)\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "\n",
    "# -------------------------\n",
    "# Preview\n",
    "# -------------------------\n",
    "chunks_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a439ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid: Sentence-Based + Fixed-Length (Token) + Overlap Chunking\n",
    "import nltk\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "CHUNK_SIZE = 100       # max tokens per chunk\n",
    "CHUNK_OVERLAP = 20     # overlapping tokens\n",
    "ENCODING_NAME = \"cl100k_base\"\n",
    "\n",
    "# -------------------------\n",
    "# Initialize\n",
    "# -------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Utility Functions\n",
    "# -------------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "# -------------------------\n",
    "# Chunk Builder\n",
    "# -------------------------\n",
    "def build_chunks(text: str) -> List[str]:\n",
    "    sentences = sent_tokenizer(clean_text(text))\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        if sentence_tokens > CHUNK_SIZE:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_tokens = 0\n",
    "            chunks.append(sentence)\n",
    "            continue\n",
    "\n",
    "        if current_tokens + sentence_tokens <= CHUNK_SIZE:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += sentence_tokens\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # handle overlap\n",
    "            overlap_text = \" \".join(current_chunk)\n",
    "            overlap_tokens = encoder.encode(overlap_text)[-CHUNK_OVERLAP:]\n",
    "            overlap_text = encoder.decode(overlap_tokens)\n",
    "\n",
    "            current_chunk = [overlap_text, sentence]\n",
    "            current_tokens = count_tokens(\" \".join(current_chunk))\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# -------------------------\n",
    "# Sample Texts\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "\n",
    "def create_stratified_sample(input_file, sample_size, stratify_col='Product', random_state=42):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(r\"C:\\Users\\hakimam\\Desktop\\pproject\\test5\\data\\processed\\filtered_feedback.csv\")\n",
    "    \n",
    "    # Calculate fraction per group\n",
    "    total_rows = len(df)\n",
    "    frac = sample_size / total_rows\n",
    "    \n",
    "    # Stratified sampling\n",
    "    df_sampled = df.groupby(stratify_col, group_keys=False).apply(\n",
    "        lambda x: x.sample(frac=frac, random_state=random_state)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return df_sampled\n",
    "\n",
    "# -------------------------\n",
    "# Apply Chunking\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Display Results\n",
    "# -------------------------\n",
    "for c in all_chunks:\n",
    "    print(f\"feedback {c['feedback_id']} - Chunk {c['chunk_index']} ({c['token_count']} tokens):\\n{c['chunk_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample chunks from previous step\n",
    "chunk_texts = [\n",
    "    \"I was charged an annual fee for my Platinum Visa card even though the promotion said it would be waived for the first year...\",\n",
    "    \"Customer service keeps transferring me between departments without providing a clear resolution...\",\n",
    "    \"Additionally, the online portal does not reflect my latest transactions correctly, which makes it difficult to verify my charges...\",\n",
    "    # ... all other chunks\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"feedback_id\": \"COMP1\", \"chunk_index\": 0, \"product_category\": \"Credit Cards\"},\n",
    "    {\"feedback_id\": \"COMP1\", \"chunk_index\": 1, \"product_category\": \"Credit Cards\"},\n",
    "    {\"feedback_id\": \"COMP1\", \"chunk_index\": 2, \"product_category\": \"Credit Cards\"},\n",
    "    # ... all other chunk metadata\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef063e50",
   "metadata": {},
   "source": [
    "# FAISS + SentenceTransformers Embeddings\n",
    "\n",
    "✅ Free, fast, works locally\n",
    "✅ Good for POC and moderate-scale datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b406cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode chunks\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index contains {index.ntotal} vectors\")\n",
    "\n",
    "# Example: Semantic search\n",
    "query = \"Why are customers unhappy with BNPL?\"\n",
    "query_vec = model.encode([query]).astype(\"float32\")\n",
    "D, I = index.search(query_vec, k=3)\n",
    "\n",
    "for i in I[0]:\n",
    "    print(metadata[i][\"chunk_index\"], chunk_texts[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a37fb6",
   "metadata": {},
   "source": [
    "# ChromaDB + SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -------------------------\n",
    "# Sample chunks and metadata\n",
    "# -------------------------\n",
    "chunk_texts = [\n",
    "    \"I was charged an annual fee for my Platinum Visa card even though the promotion said it would be waived for the first year...\",\n",
    "    \"Customer service keeps transferring me between departments without providing a clear resolution...\",\n",
    "    \"Additionally, the online portal does not reflect my latest transactions correctly, which makes it difficult to verify my charges...\"\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"complaint_id\": \"COMP1\", \"chunk_index\": 0, \"product_category\": \"Credit Cards\"},\n",
    "    {\"complaint_id\": \"COMP1\", \"chunk_index\": 1, \"product_category\": \"Credit Cards\"},\n",
    "    {\"complaint_id\": \"COMP1\", \"chunk_index\": 2, \"product_category\": \"Credit Cards\"}\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Initialize ChromaDB (New API)\n",
    "# -------------------------\n",
    "client = chromadb.Client(Settings(\n",
    "    persist_directory=\"./chroma_db\",   # local storage\n",
    "    anonymized_telemetry=False          # optional, disables telemetry\n",
    "))\n",
    "\n",
    "# Create collection (or get if exists)\n",
    "collection_name = \"complaints\"\n",
    "collections = client.list_collections()\n",
    "if collection_name in [c.name for c in collections]:\n",
    "    collection = client.get_collection(name=collection_name)\n",
    "else:\n",
    "    collection = client.create_collection(name=collection_name)\n",
    "\n",
    "# -------------------------\n",
    "# SentenceTransformer embeddings\n",
    "# -------------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "embeddings = [emb.tolist() for emb in embeddings]  # Chroma requires list of floats\n",
    "\n",
    "# -------------------------\n",
    "# Add chunks to collection\n",
    "# -------------------------\n",
    "for i, emb in enumerate(embeddings):\n",
    "    collection.add(\n",
    "        documents=[chunk_texts[i]],\n",
    "        embeddings=[emb],\n",
    "        metadatas=[metadata[i]],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Added {len(chunk_texts)} chunks to ChromaDB collection '{collection_name}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f99c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Query example\n",
    "# -------------------------\n",
    "query = \"Why are customers frustrated with credit cards?\"\n",
    "query_emb = model.encode([query])[0].tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=2,\n",
    "    where={\"product_category\": \"Credit Cards\"}\n",
    ")\n",
    "\n",
    "print(\"Query Results:\")\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(f\"- {meta['complaint_id']} / Chunk {meta['chunk_index']}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272679a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5545c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Add chunks to vector store\n",
    "# -------------------------\n",
    "for i, emb in enumerate(embeddings):\n",
    "    collection.add(\n",
    "        documents=[chunk_texts[i]],\n",
    "        embeddings=[emb],\n",
    "        metadatas=[metadata[i]],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "\n",
    "print(f\"✅ Indexed {len(chunk_texts)} chunks into ChromaDB collection '{collection_name}'\")\n",
    "\n",
    "# -------------------------\n",
    "# Example semantic query\n",
    "# -------------------------\n",
    "query = \"Why are customers frustrated with credit cards?\"\n",
    "query_emb = model.encode([query])[0].tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=2,\n",
    "    where={\"product_category\": \"Credit Cards\"}  # optional filter\n",
    ")\n",
    "\n",
    "print(\"Top search results:\")\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(f\"- {meta['complaint_id']} / Chunk {meta['chunk_index']}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dd8ee",
   "metadata": {},
   "source": [
    "### 1. Load the full CFPB complaint dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ef2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load raw complaints\n",
    "df = pd.read_csv(\"../data/raw/complaints-2025.csv\")\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d370ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ebb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify Columns by Importance\n",
    "#'''critical_columns (Must NOT be null)'''\n",
    "#'''Consumer complaint narrative --❗ Drop rows where this is null\n",
    "#Complaint ID, product,  date received'''\n",
    "# If narrative is null → no chunking, no embedding\n",
    "#'''Important Metadata (Can be null): Used for filtering, trends, clustering, but not required.'''\n",
    "#'''Sub-product', 'Issue', 'Sub-issue', 'State',  'Tags', 'Consumer consent provided?',  'Date sent to company',   'Consumer disputed?''' \n",
    "#so fill with unknown\n",
    "# others are Useful for dashboards, not semantic meaning.(Not for Embeddings)\n",
    "#'''Only embed rows with a non-null Consumer complaint narrative'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258efc21",
   "metadata": {},
   "source": [
    "### 2. Cleaning data set\n",
    "\n",
    "Drop rows with null complaint text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d71752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with null complaint text\n",
    "df = df.dropna(subset=[\"Consumer complaint narrative\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cleaning Function\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_normalize_text(text):\n",
    "    # Handle None / NaN safely\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, float) and pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    # Normalize unicode (smart quotes, etc.)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "\n",
    "    # Remove boilerplate / disclaimers\n",
    "    text = re.sub(r\"this message is confidential.*\", \" \", text, flags=re.I)\n",
    "    text = re.sub(r\"please do not reply to this email.*\", \" \", text, flags=re.I)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & normalize complaint text\n",
    "\n",
    "df[\"Consumer complaint narrative\"] = (\n",
    "    df[\"Consumer complaint narrative\"]\n",
    "    .astype(str)\n",
    "    .apply(clean_and_normalize_text)\n",
    ")\n",
    "\n",
    "# Remove empty text after cleaning\n",
    "df = df[df[\"Consumer complaint narrative\"].str.strip().astype(bool)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the result\n",
    "print(df[\"Consumer complaint narrative\"].head())\n",
    "print(f\"Remaining complaints: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdca672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1124cfe9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603b372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Splitting:  split complaints into sentences which is best for semantic chunking.\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_split(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "df[\"sentences\"] = df[\"Consumer complaint narrative\"].apply(sentence_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (Token Statistics): to measure real LLM token sizes.\n",
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def token_count(text):\n",
    "    return len(encoder.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1815351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Sentence Lengths: helps decide chunk size scientifically, not by guessing.\n",
    "sentence_tokens = []\n",
    "\n",
    "for sentences in df[\"sentences\"]:\n",
    "    for s in sentences:\n",
    "        sentence_tokens.append(token_count(s))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Avg tokens per sentence:\", int(np.mean(sentence_tokens)))\n",
    "print(\"95th percentile:\", int(np.percentile(sentence_tokens, 95)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Complaints (Sentence-Based + Overlap)\n",
    "def chunk_sentences(sentences, chunk_size, overlap):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = token_count(sentence)\n",
    "\n",
    "        if current_tokens + tokens <= chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += tokens\n",
    "        else:\n",
    "            # save chunk\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # overlap\n",
    "            overlap_tokens = encoder.encode(\" \".join(current_chunk))[-overlap:]\n",
    "            overlap_text = encoder.decode(overlap_tokens)\n",
    "\n",
    "            current_chunk = [overlap_text, sentence]\n",
    "            current_tokens = token_count(\" \".join(current_chunk))\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 120\n",
    "CHUNK_OVERLAP = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff486d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chunking\n",
    "\n",
    "chunk_rows = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_sentences(\n",
    "        row[\"sentences\"],\n",
    "        CHUNK_SIZE,\n",
    "        CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_rows.append({\n",
    "            \"complaint_id\": row[\"Complaint ID\"],\n",
    "            \"product\": row[\"Product\"],\n",
    "            \"issue\": row[\"Issue\"],\n",
    "            \"state\": row[\"State\"],\n",
    "            \"date_received\": row[\"Date received\"],\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": chunk\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunks to DataFrame\n",
    "import pandas as pd\n",
    "chunk_df = pd.DataFrame(chunk_rows)\n",
    "print(\"Total chunks:\", len(chunk_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index into ChromaDB (Vector Store)\n",
    "# Initialize ChromaDB\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.Client(Settings(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    anonymized_telemetry=False\n",
    "))\n",
    "\n",
    "collection = client.get_or_create_collection(\"complaints\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90258714",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = model.encode(\n",
    "    chunk_df[\"chunk_text\"].tolist(),\n",
    "    show_progress_bar=True\n",
    ").tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1834a",
   "metadata": {},
   "source": [
    "## Add to Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5000  # Safe under ChromaDB limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c645cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "total = len(chunk_df)\n",
    "num_batches = ceil(total / BATCH_SIZE)\n",
    "\n",
    "print(f\"Indexing {total} chunks in {num_batches} batches...\")\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, total)\n",
    "\n",
    "    batch_docs = chunk_df[\"chunk_text\"].iloc[start:end].tolist()\n",
    "    batch_embeddings = embeddings[start:end]\n",
    "    batch_metadata = (\n",
    "        chunk_df\n",
    "        .drop(columns=[\"chunk_text\"])\n",
    "        .iloc[start:end]\n",
    "        .to_dict(\"records\")\n",
    "    )\n",
    "    batch_ids = [f\"chunk_{j}\" for j in range(start, end)]\n",
    "\n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadata,\n",
    "        ids=batch_ids\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Batch {i+1}/{num_batches} indexed ({end-start} vectors)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67237caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Semantic Search\n",
    "\n",
    "#Encodes the query: Converts the question into a dense semantic vector\n",
    "query = \"Why are customers unhappy with credit cards?\"\n",
    "query_embedding = model.encode([query])[0].tolist()\n",
    "# Queries ChromaDB with a filtered by product type\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5,\n",
    "    where={\"product\": \"Credit card\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea242656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c557c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints top 5 most similar chunks\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(meta[\"complaint_id\"], \"→\", doc[:150], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6905f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print full chunk (easy)\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(meta[\"complaint_id\"], \"→\", doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4765d",
   "metadata": {},
   "source": [
    "## Different stratagies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ee874",
   "metadata": {},
   "source": [
    "### 1. Fixed-Length Chunking (Character-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8k = df.sample(n=8000, random_state=42)  # random_state for reproducibility\n",
    "df_clean_8k = df_8k.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"data/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_dir, \"complaints_8k.csv\")\n",
    "df_8k.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14070b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saved {len(df_8k)} records to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df is your original DataFrame\n",
    "one_third = df.iloc[:len(df) // 3]\n",
    "\n",
    "# save as new data\n",
    "one_third.to_csv(\"df_one_third.csv\", index=False)\n",
    "\n",
    "df_one_third = pd.read_csv(\"df_one_third.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fixed_length_chunking(df, chunk_size=500):\n",
    "    chunks = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"Consumer complaint narrative\"]\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunks.append({\n",
    "                \"id\": f\"{row['Complaint ID']}_fixed_{i}\",\n",
    "                \"text\": text[i:i+chunk_size],\n",
    "                \"metadata\": {\n",
    "                    \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                    \"Product\": row[\"Product\"],\n",
    "                    \"strategy\": \"fixed_length\"\n",
    "                }\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "\n",
    "fixed_chunks_df = fixed_length_chunking(df_8k , chunk_size=500)\n",
    "fixed_chunks_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2659dba",
   "metadata": {},
   "source": [
    "### 2. Sentence-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_chunking(df, max_chars=500):\n",
    "    chunks = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"Consumer complaint narrative\"]\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunk_text = \"\"\n",
    "        chunk_id = 0\n",
    "\n",
    "        for sent in sentences:\n",
    "            if len(chunk_text) + len(sent) <= max_chars:\n",
    "                chunk_text += \" \" + sent\n",
    "            else:\n",
    "                chunks.append({\n",
    "                    \"id\": f\"{row['Complaint ID']}_sentence_{chunk_id}\",\n",
    "                    \"text\": chunk_text.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                        \"Product\": row[\"Product\"],\n",
    "                        \"strategy\": \"sentence\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_text = sent\n",
    "                chunk_id += 1\n",
    "\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"id\": f\"{row['Complaint ID']}_sentence_{chunk_id}\",\n",
    "                \"text\": chunk_text.strip(),\n",
    "                \"metadata\": {\n",
    "                    \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                    \"Product\": row[\"Product\"],\n",
    "                    \"strategy\": \"sentence\"\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return chunks\n",
    "sentence_chunks_df = sentence_based_chunking(df_8k , max_chars=500)\n",
    "sentence_chunks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98935a2",
   "metadata": {},
   "source": [
    "### 3. Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunking(df, max_chars=500):\n",
    "    chunks = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row[\"Consumer complaint narrative\"]\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        chunk_id = 0\n",
    "\n",
    "        for para in paragraphs:\n",
    "            if len(para) <= max_chars:\n",
    "                chunks.append({\n",
    "                    \"id\": f\"{row['Complaint ID']}_recursive_{chunk_id}\",\n",
    "                    \"text\": para.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                        \"Product\": row[\"Product\"],\n",
    "                        \"strategy\": \"recursive\"\n",
    "                    }\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            else:\n",
    "                sentences = sent_tokenize(para)\n",
    "                current = \"\"\n",
    "\n",
    "                for sent in sentences:\n",
    "                    if len(current) + len(sent) <= max_chars:\n",
    "                        current += \" \" + sent\n",
    "                    else:\n",
    "                        chunks.append({\n",
    "                            \"id\": f\"{row['Complaint ID']}_recursive_{chunk_id}\",\n",
    "                            \"text\": current.strip(),\n",
    "                            \"metadata\": {\n",
    "                                \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                                \"Product\": row[\"Product\"],\n",
    "                                \"strategy\": \"recursive\"\n",
    "                            }\n",
    "                        })\n",
    "                        current = sent\n",
    "                        chunk_id += 1\n",
    "\n",
    "                if current:\n",
    "                    chunks.append({\n",
    "                        \"id\": f\"{row['Complaint ID']}_recursive_{chunk_id}\",\n",
    "                        \"text\": current.strip(),\n",
    "                        \"metadata\": {\n",
    "                            \"Complaint ID\": row[\"Complaint ID\"],\n",
    "                            \"Product\": row[\"Product\"],\n",
    "                            \"strategy\": \"recursive\"\n",
    "                        }\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "\n",
    "    return chunks\n",
    "recursive_chunks_df = recursive_chunking(df_8k, max_chars=500)\n",
    "recursive_chunks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d94f78",
   "metadata": {},
   "source": [
    "### 4. Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# load embeding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # nice progress bar\n",
    "\n",
    "# Example: embedding_model must support batch encoding\n",
    "# embedding_model.encode([\"list\", \"of\", \"sentences\"], batch_size=64)\n",
    "\n",
    "def fast_semantic_chunking(df, embedding_model, similarity_threshold=0.75, batch_size=64):\n",
    "    \"\"\"\n",
    "    Fast semantic chunking using batch embeddings and vectorized operations.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Consumer complaint narrative' and 'Complaint ID'.\n",
    "        embedding_model: Model with a .encode() method supporting batch input.\n",
    "        similarity_threshold: Threshold to merge sentences into chunks.\n",
    "        batch_size: Batch size for embedding computation.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with semantic chunks and metadata.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    texts = df[\"Consumer complaint narrative\"].fillna(\"\").tolist()\n",
    "    ids = df[\"Complaint ID\"].tolist()\n",
    "    products = df[\"Product\"].tolist()\n",
    "\n",
    "    for text, cid, product in tqdm(zip(texts, ids, products), total=len(texts)):\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # Compute embeddings in batches\n",
    "        embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "            batch_embeddings = embedding_model.encode(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        current_text = sentences[0]\n",
    "        current_embedding = embeddings[0]\n",
    "        chunk_id = 0\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(\n",
    "                current_embedding.reshape(1, -1),\n",
    "                embeddings[i].reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                current_text += \" \" + sentences[i]\n",
    "                current_embedding = np.mean([current_embedding, embeddings[i]], axis=0)\n",
    "            else:\n",
    "                all_chunks.append({\n",
    "                    \"id\": f\"{cid}_semantic_{chunk_id}\",\n",
    "                    \"text\": current_text.strip(),\n",
    "                    \"Complaint ID\": cid,\n",
    "                    \"Product\": product,\n",
    "                    \"strategy\": \"semantic\"\n",
    "                })\n",
    "                current_text = sentences[i]\n",
    "                current_embedding = embeddings[i]\n",
    "                chunk_id += 1\n",
    "\n",
    "        # Append last chunk\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"{cid}_semantic_{chunk_id}\",\n",
    "            \"text\": current_text.strip(),\n",
    "            \"Complaint ID\": cid,\n",
    "            \"Product\": product,\n",
    "            \"strategy\": \"semantic\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(all_chunks)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a063e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fast_semantic_chunking(\n",
    "    df,\n",
    "    embedding_model,\n",
    "    similarity_threshold=0.75,\n",
    "    batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast semantic chunking using batch embeddings and vectorized operations.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict] — directly compatible with add_to_chroma\n",
    "    \"\"\"\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    texts = df[\"Consumer complaint narrative\"].fillna(\"\").tolist()\n",
    "    ids = df[\"Complaint ID\"].tolist()\n",
    "    products = df[\"Product\"].tolist()\n",
    "\n",
    "    for text, cid, product in tqdm(zip(texts, ids, products), total=len(texts)):\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        if not sentences:\n",
    "            continue\n",
    "\n",
    "        # ---- Batch embedding ----\n",
    "        embeddings = []\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i + batch_size]\n",
    "            batch_embeddings = embedding_model.encode(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "\n",
    "        # ---- Chunk construction ----\n",
    "        current_text = sentences[0]\n",
    "        current_embedding = embeddings[0]\n",
    "        chunk_id = 0\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(\n",
    "                current_embedding.reshape(1, -1),\n",
    "                embeddings[i].reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                current_text += \" \" + sentences[i]\n",
    "                current_embedding = np.mean(\n",
    "                    [current_embedding, embeddings[i]], axis=0\n",
    "                )\n",
    "            else:\n",
    "                all_chunks.append({\n",
    "                    \"id\": f\"{cid}_semantic_{chunk_id}\",\n",
    "                    \"text\": current_text.strip(),\n",
    "                    \"metadata\": {\n",
    "                        \"Complaint ID\": cid,\n",
    "                        \"Product\": product,\n",
    "                        \"strategy\": \"semantic\"\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                current_text = sentences[i]\n",
    "                current_embedding = embeddings[i]\n",
    "                chunk_id += 1\n",
    "\n",
    "        # ---- Final chunk ----\n",
    "        all_chunks.append({\n",
    "            \"id\": f\"{cid}_semantic_{chunk_id}\",\n",
    "            \"text\": current_text.strip(),\n",
    "            \"metadata\": {\n",
    "                \"Complaint ID\": cid,\n",
    "                \"Product\": product,\n",
    "                \"strategy\": \"semantic\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f80e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  embedding_model is SentenceTransformer \n",
    "chunks_df = fast_semantic_chunking(df, embedding_model, similarity_threshold=0.75, batch_size=64)\n",
    "# print(chunks_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ab9af",
   "metadata": {},
   "source": [
    "### Create ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9510c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bd7f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(collection_name, chunks, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Add chunks to a Chroma collection in batches to avoid max batch size errors.\n",
    "    \"\"\"\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        collection.add(\n",
    "            ids=[c[\"id\"] for c in batch],\n",
    "            documents=[c[\"text\"] for c in batch],\n",
    "            metadatas=[c[\"metadata\"] for c in batch]\n",
    "        )\n",
    "\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41327fdb",
   "metadata": {},
   "source": [
    "**Build All Indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fcffb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_collection = add_to_chroma(\n",
    "    \"fixed_length\",\n",
    "    fixed_length_chunking(df_8k , chunk_size=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe43d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_collection = add_to_chroma(\n",
    "    \"sentence_based\",\n",
    "    sentence_based_chunking(df_8k , max_chars=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c073306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recursive_collection = add_to_chroma(\n",
    "    \"recursive\",\n",
    "    recursive_chunking(df_8k, max_chars=500)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577aba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_collection = add_to_chroma(\n",
    "    \"semantic\",\n",
    "    fast_semantic_chunking(df_8k, embedding_model, similarity_threshold=0.75)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87c2ed",
   "metadata": {},
   "source": [
    "**Query  Strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(collection, query, top_k=5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"chunk\": results[\"documents\"][0],\n",
    "        \"Complaint ID\": [m[\"Complaint ID\"] for m in results[\"metadatas\"][0]],\n",
    "        \"Product\": [m[\"Product\"] for m in results[\"metadatas\"][0]],\n",
    "        \"strategy\": [m[\"strategy\"] for m in results[\"metadatas\"][0]]\n",
    "    })\n",
    "query = \"Why are customers unhappy with credit cards?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63af5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed-Length Query\n",
    "fixed_results = query_collection(\n",
    "    fixed_collection,\n",
    "    query\n",
    ")\n",
    "fixed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentence-Based Query\n",
    "\n",
    "sentence_results = query_collection(\n",
    "    sentence_collection,\n",
    "    query\n",
    ")\n",
    "sentence_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Recursive Query\n",
    "recursive_results = query_collection(\n",
    "    recursive_collection,\n",
    "    query\n",
    ")\n",
    "recursive_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841465b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Semantic Query\n",
    "semantic_results = query_collection(\n",
    "    semantic_collection,\n",
    "    query\n",
    ")\n",
    "semantic_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-Side Comparison\n",
    "comparison_df = pd.concat(\n",
    "    [\n",
    "        fixed_results,\n",
    "        sentence_results,\n",
    "        recursive_results,\n",
    "        semantic_results\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want Query only Credit Card complaints:\n",
    "semantic_collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5,\n",
    "    where={\"Product\": \"Credit card\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da2606",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
